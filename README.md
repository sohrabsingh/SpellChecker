# Hindi Spell Checker (Seq2Seq) üöÄ

![Python](https://img.shields.io/badge/python-3.12-blue)
![PyTorch](https://img.shields.io/badge/pytorch-2.1-red)
![License](https://img.shields.io/badge/license-MIT-green)

A PyTorch-based sequence-to-sequence model for **Hindi spell correction**. This project generates typos, builds a vocabulary, trains a Seq2Seq model with scheduled teacher forcing, and evaluates predictions. Optimized for Windows and low-VRAM GPUs (e.g., NVIDIA 3050 4GB).

---

## Table of Contents

- [Features](#features)  
- [Project Structure](#project-structure)  
- [Setup](#setup)  
- [Usage](#usage)  
- [Model Details](#model-details)  
- [Checkpoint Loading](#optional-checkpoint-loading)  
- [Example Output](#example-output)  
- [License](#license)  
- [Author](#author)  

---

## Features

- Synthetic typo generation for Hindi text.
- Character-level Seq2Seq model (LSTM encoder-decoder).  
- Scheduled teacher forcing during training.  
- Supports GPU acceleration via CUDA.  
- Checkpointing & model saving.  
- Evaluation with token-level accuracy and sample predictions.  
- Windows & low-VRAM friendly.

---

## Project Structure

```

.
‚îú‚îÄ‚îÄ data/                  # Raw and processed data
‚îÇ   ‚îî‚îÄ‚îÄ all_hindi_clean.txt
‚îú‚îÄ‚îÄ vocab/                 # Vocabulary files
‚îú‚îÄ‚îÄ checkpoints/           # Model checkpoints
‚îú‚îÄ‚îÄ InitiallyOkay.ipynb    # Source code
‚îú‚îÄ‚îÄ encoder_state_dict.h5
‚îú‚îÄ‚îÄ decoder_state_dict.h5
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ requirements.txt

````

---

## Setup

### 1. Clone the repo
```bash
git clone <repo_url>
cd <repo_name>
````

### 2. Install dependencies

```bash
pip install -r requirements.txt
```

### 3. Ensure CUDA support (optional)

```python
import torch
torch.cuda.is_available()
```

---

## Usage

### 1. Prepare Data

Place your Hindi corpus in:

```
data/all_hindi_clean.txt
```

The script automatically generates typo-target pairs.

### 2. Train the Model

* Default: 10 epochs
* Batch size: 16 (gradient accumulation simulates 32)
* Scheduled teacher forcing decays from 1.0 ‚Üí 0.5
* Best checkpoint saved in `checkpoints/seq2seq_best.pt`

### 3. Evaluate & Sample Predictions

* Outputs average loss & token-level accuracy.
* Displays sample input ‚Üí target ‚Üí predicted sequences.

---

## Model Details

* **Encoder:** LSTM, embedding + dropout
* **Decoder:** LSTM, embedding + dropout + linear output
* **Loss:** CrossEntropyLoss (ignores `<PAD>`)
* **Optimizer:** Adam
* **Batch size:** 16 (configurable)
* **Gradient accumulation:** 2 steps

---

## Optional Checkpoint Loading

```python
checkpoint = torch.load("checkpoints/seq2seq_epoch3.pt", map_location=device)
model.load_state_dict(checkpoint["model_state"])
```

---

## Example Output

```
Input     : ‡§Æ‡§®‡•á ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ‡§®‡§æ ‡§π‡•á
Target    : ‡§Æ‡•Å‡§ù‡•á ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ‡§®‡§æ ‡§π‡•à
Predicted : ‡§Æ‡•Å‡§ù‡•á ‡§∏‡•ç‡§ï‡•Ç‡§≤ ‡§ú‡§æ‡§®‡§æ ‡§π‡•à
------------------------------------------------------------
Input     : ‡§µ‡§π ‡§¨‡§æ‡§ú‡§º‡§æ‡§∞ ‡§Æ‡•á ‡§ó‡§Ø‡§æ
Target    : ‡§µ‡§π ‡§¨‡§æ‡§ú‡§º‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§ó‡§Ø‡§æ
Predicted : ‡§µ‡§π ‡§¨‡§æ‡§ú‡§º‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§ó‡§Ø‡§æ
```

---

## License

MIT License ‚Äì free for academic and personal use.

---

## Author

**Sohrab Singh**



